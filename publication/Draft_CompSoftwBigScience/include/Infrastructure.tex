Virtualization has become mainstream technology over the last decade as it allows
both to host more than one operating system on a single server and to strictly
separate users of software environments. While widespread in computer center
operation it is still sceptically seen in the field of scientific computing~\cite{VirtualisationScientificComp} and
thus rarely applied in HPC.

\subsection{Computing at the University of Freiburg computer center}
% Infrastructure setting

The computer center at University of Freiburg (UFR) provides
reasonably scaled [intermediate scale?] research
infrastructures like cloud, storage and especially HPC services to cater to the
needs of various scientific communities. To operate compute systems comprised of
more than 1000 inidividual nodes with a small group of administrators
significant standardization in hardware and software is necessary.

The level of granularity of the software stack provided is not fine enough to
directly support the quite special requirements of world-wide efforts like the
ATLAS or CMS experiments in the field of particle physics. To utilize the system
optimally and open the cluster to as many different needs as possible without
overstretching the operating groups own man power novel approaches to the
utilization of the installed hardware were necessary. As virtualization was
already in use for the operation of the local private cloud, the eScience group
pooled in it's expertise in cloud operation especially the use of OpenStack as a
cloud platform to provide a more flexible software deployment on the cluster
beside the existing software module system. This produced a couple of
implications ranging from challenges in the automated creation of suitable
virtual machines, their on-demoand deployment and the scheduling on the cluster
and virtual resources level.

\subsection{Separation of software environments}

The filesystem of a virtual machine (or VRE in the described usecase) is a
container presented as a single file. From the computer center's perspective
this container is seen as a black box requiring no involvement or efforts like
updates of the operating system or the provisioning of software packages of a
certain version. From the researchers perspective the VRE is an individual
(virtual) server where everything from the hardware level -- at least to a
certain degree like CPU or RAM dimensioning -- up to the operating system,
applications and configurations can be controlled solely by the research groups.

To allow more flexible software environments the standard bare metal operation
the NEMO cluster got extended with a parallel installation of OpenStack
components \cite{hpc-symp:2016}. They are getting orchestrated from a special
management node which provides the necessary interfaces to the other services.
The NEMO cluster as it's siblings throughout the state use Adaptive's MOAB as a
scheduler of compute jobs. To allow seemless coexistence of both bare metal and
virtualied jobs the scheduler has to handle both of them. As the scheduler does
not look into the VRE virtual machines, it cannot assess if they are actually
active or idle. To solve this, a customized glue component got introduced (to be
described in further detail in section XX). The virtual machines started and
stopped by the OpenStack nova(?) components triggered by ??.

Michael fügt hier hoch was am Montag hinzu, da er leider letze Woche gekränket
hat ;)

